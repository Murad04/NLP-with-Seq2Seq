{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnAKiTQQhhkvb4kvmIN0/P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as NN\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9inJd8Wd9OyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = \"/content/data (1).zip\"\n",
        "\n",
        "# Open the zip file in read mode\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  # Extract all files to the current directory\n",
        "  zip_ref.extractall()\n",
        "\n",
        "  print(\"Extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYFcK6cDzgv",
        "outputId": "35db817b-9638-420f-d3e9-51827ddf6e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token=0\n",
        "EOS_token=1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.word2index={}\n",
        "    self.word2count={}\n",
        "    self.index2word={0:\"SOS\",1:\"EOS\"}\n",
        "    self.n_words=2\n",
        "\n",
        "  def addSentence(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self,word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word]=self.n_words\n",
        "      self.word2count[word]=1\n",
        "      self.index2word[self.n_words]=word\n",
        "      self.n_words+=1\n",
        "    else:\n",
        "      self.word2count[word]+=1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ADA_CevrEWOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToASCII(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD',s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s=unicodeToASCII(s.lower().strip())\n",
        "  s=re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "  return s.strip()"
      ],
      "metadata": {
        "id": "iiOlw7L5FiFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1,lang2,reverse=False):\n",
        "  print(\"Reading lines..\")\n",
        "  lines=open('data/%s-%s.txt' % (lang1,lang2),encoding='utf-8').read().strip().split('\\n')\n",
        "  pairs=[[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "  if reverse:\n",
        "    pairs=[list(reversed(p)) for p in pairs]\n",
        "    input_lang=Lang(lang2)\n",
        "    output_lang=Lang(lang1)\n",
        "  else:\n",
        "    input_lang=Lang(lang1)\n",
        "    output_lang=Lang(lang2)\n",
        "\n",
        "  return input_lang,output_lang,pairs\n"
      ],
      "metadata": {
        "id": "JESWsfxLF24o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH=10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "EyG98JPvGqtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1,lang2,reverse=False):\n",
        "  input_lang,output_lang,pairs=readLangs(lang1,lang2,reverse)\n",
        "  pairs=filterPairs(pairs)\n",
        "  for pair in pairs:\n",
        "    input_lang.addSentence(pair[0])\n",
        "    output_lang.addSentence(pair[1])\n",
        "  print(\"Counted words:\")\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA8ZugGIG4gG",
        "outputId": "b40a6ce4-a8b4-4019-b5fa-3428a946c2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines..\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "['elle est sourde a mes conseils', 'she is deaf to my advice']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(NN.Module):\n",
        "  def __init__(self,input_size,hidden_size,dropout_p=0.1):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "    self.embedding=NN.Embedding(input_size,hidden_size)\n",
        "    self.gru=NN.GRU(hidden_size,hidden_size,batch_first=True)\n",
        "    self.dropout=NN.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self,input):\n",
        "    embedded=self.dropout(self.embedding(input))\n",
        "    output,hidden=self.gru(embedded)\n",
        "    return output,hidden"
      ],
      "metadata": {
        "id": "moc5SKoVHXKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(NN.Module):\n",
        "  def __init__(self,hidden_size,output_size):\n",
        "    super(DecoderRNN,self).__init__()\n",
        "    self.embedding=NN.Embedding(output_size,hidden_size)\n",
        "    self.gru=NN.GRU(hidden_size,hidden_size,batch_first=True)\n",
        "    self.out=NN.Linear(hidden_size,output_size)\n",
        "\n",
        "  def forward(self,encoder_outputs,encoder_hidden,target_tensor=None):\n",
        "    batch_size=encoder_outputs.size(0)\n",
        "    decoder_input=torch.empty(batch_size,1,dtype=torch.long,device=device).fill_(SOS_token)\n",
        "    decoder_hidden=encoder_hidden\n",
        "    decoder_outputs=[]\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "      decoder_output,decoder_hidden=self.forward_step(decoder_input,decoder_hidden)\n",
        "      decoder_outputs.append(decoder_output)\n",
        "\n",
        "      if target_tensor is not None:\n",
        "        decoder_input=target_tensor[:,i].unsqueeze(1)\n",
        "      else:\n",
        "        _,topi=decoder_output.topk(1)\n",
        "        decoder_input=topi.squeeze(-1).detach()\n",
        "\n",
        "\n",
        "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "    return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "  def forward_step(self, input, hidden):\n",
        "    output = self.embedding(input)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.out(output)\n",
        "    return output, hidden\n"
      ],
      "metadata": {
        "id": "9HF-DJBEIdNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(NN.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = NN.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = NN.Linear(hidden_size, hidden_size)\n",
        "        self.Va = NN.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(NN.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = NN.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = NN.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = NN.Linear(hidden_size, output_size)\n",
        "        self.dropout = NN.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "rbqZltiyL9Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ],
      "metadata": {
        "id": "kkFDmXrnMDHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "uWYcJVahMHJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "4_2O91GFMJWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = NN.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "metadata": {
        "id": "tExZFdW3ML22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "yaN29DtVMTYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "FwG3aTs9MVFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "0xlJCv3PMX7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDCeQCcDMe6B",
        "outputId": "f4b7bd11-efb4-410e-f354-89ceaec256d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines..\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "0m 36s (- 9m 14s) (5 6%) 1.5620\n",
            "1m 12s (- 8m 28s) (10 12%) 0.6935\n",
            "1m 48s (- 7m 51s) (15 18%) 0.3527\n",
            "2m 24s (- 7m 13s) (20 25%) 0.1915\n",
            "3m 0s (- 6m 36s) (25 31%) 0.1167\n",
            "3m 35s (- 5m 58s) (30 37%) 0.0803\n",
            "4m 10s (- 5m 22s) (35 43%) 0.0619\n",
            "4m 46s (- 4m 46s) (40 50%) 0.0505\n",
            "5m 21s (- 4m 10s) (45 56%) 0.0437\n",
            "5m 57s (- 3m 34s) (50 62%) 0.0400\n",
            "6m 37s (- 3m 0s) (55 68%) 0.0357\n",
            "7m 20s (- 2m 26s) (60 75%) 0.0343\n",
            "7m 57s (- 1m 50s) (65 81%) 0.0320\n",
            "8m 34s (- 1m 13s) (70 87%) 0.0306\n",
            "9m 9s (- 0m 36s) (75 93%) 0.0299\n",
            "9m 44s (- 0m 0s) (80 100%) 0.0283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder,decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ii-Zn4MgkO",
        "outputId": "4d69f9ba-80ba-44e0-d866-66c0fca6c7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> elle vous attend a la maison\n",
            "= she s waiting for you at home\n",
            "< she s waiting for you at home <EOS>\n",
            "\n",
            "> je suis pret a y aller\n",
            "= i m ready to go\n",
            "< i am ready to go <EOS>\n",
            "\n",
            "> il fait plus de kilos\n",
            "= he is over kilos\n",
            "< he is over kilos <EOS>\n",
            "\n",
            "> je ne suis pas interessee par le gain materiel\n",
            "= i am not interested in material gain\n",
            "< i am not interested in material gain <EOS>\n",
            "\n",
            "> nous ne sommes pas des flics\n",
            "= we re not cops\n",
            "< we re not cops <EOS>\n",
            "\n",
            "> nous voyageons a petit budget\n",
            "= we are traveling on a tight budget\n",
            "< we are traveling on a tight budget <EOS>\n",
            "\n",
            "> je suis un idiot complet\n",
            "= i m a complete idiot\n",
            "< i m a complete idiot <EOS>\n",
            "\n",
            "> elles sont toutes normales\n",
            "= they re all normal\n",
            "< they re all normal <EOS>\n",
            "\n",
            "> tu es plus intelligent que moi\n",
            "= you re smarter than me\n",
            "< you re smarter than me <EOS>\n",
            "\n",
            "> elle est son amie\n",
            "= she is his friend\n",
            "< she is her friend <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "\n",
        "\n",
        "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
        "\n",
        "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
        "\n",
        "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
        "\n",
        "evaluateAndShowAttention('je suis reellement fiere de vous')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yysN7Vh5PEmL",
        "outputId": "40900397-24b7-449b-8b15-fdf5e2941c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = il n est pas aussi grand que son pere\n",
            "output = he is not as tall as his father <EOS>\n",
            "input = je suis trop fatigue pour conduire\n",
            "output = i m too tired to drive <EOS>\n",
            "input = je suis desole si c est une question idiote\n",
            "output = i m sorry if this is a stupid question <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-332b9c61fc98>:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "<ipython-input-44-332b9c61fc98>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = je suis reellement fiere de vous\n",
            "output = i m really proud of you <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKDBSsjPJa2",
        "outputId": "877d17ee-f436-4c29-b206-93cd99da5e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderRNN(\n",
              "  (embedding): Embedding(4601, 128)\n",
              "  (gru): GRU(128, 128, batch_first=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder"
      ],
      "metadata": {
        "id": "umhRApYUQNRx",
        "outputId": "def55524-c4e0-460b-ef83-00930cfa1016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AttnDecoderRNN(\n",
              "  (embedding): Embedding(2991, 128)\n",
              "  (attention): BahdanauAttention(\n",
              "    (Wa): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (Ua): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (Va): Linear(in_features=128, out_features=1, bias=True)\n",
              "  )\n",
              "  (gru): GRU(256, 128, batch_first=True)\n",
              "  (out): Linear(in_features=128, out_features=2991, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAikXjulQOXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}